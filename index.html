
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
	
<!-- Title and CSS -->
<head profile="http://www.w3.org/2005/10/profile">
<link href="https://fonts.googleapis.com/css?family=B612|Lato|Public+Sans|Rubik|Source+Sans+Pro&display=swap" rel="stylesheet">

	<link rel="stylesheet" type="text/css"
	href="style.css" />
	<title>Virginia Smith</title>
	
	<!-- Google Analytics -->
	<script type="text/javascript" src="https://gc.kis.v2.scr.kaspersky-labs.com/FD126C42-EBFA-4E12-B309-BB3FDD723AC1/main.js?attr=3w-U_EeiCQQ9JzWh9lXIPIVzcFf9LwaFXbJsBiQSb9t1ejn38LylByQkd9eSpnrQcAU2QzSFXhHynIKlYUvSzQ" charset="UTF-8"></script><link rel="stylesheet" crossorigin="anonymous" href="https://gc.kis.v2.scr.kaspersky-labs.com/E3E8934C-235A-4B0E-825A-35A08381A191/abn/main.css?attr=aHR0cHM6Ly93d3cuY3MuY211LmVkdS9-c21pdGh2Lw"/><script type="text/javascript">
	  var _gaq = _gaq || [];
	  _gaq.push(['_setAccount', 'UA-35185612-1']);
	  _gaq.push(['_trackPageview']);
	
	  (function() {
		var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
		ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
		var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
	  })();
	</script>

	<!-- Show/Hide Div -->
	<script>
	function toggle(showHideDiv) {
  		var x = document.getElementById(showHideDiv);
  		if (x.style.display === "none") {
    		x.style.display = "block";
  		} else {
    		x.style.display = "none";
  		}
	}
	</script>
</head>


<!-- Body -->
<body>
<div id="wrapper">

<!-- Header & Navigation -->
<div id="masterheader">
	<div id="name">
		<font color="989898">Virginia</font> <font color="404040">Smith</font>
	</div>
	<div id="navigation">
		<a class="reg" href="#research">Research</a> <font face="Symbol">&#183;</font>
		<a class="reg" href="bio.html" target="_blank">Bio</a> <font face="Symbol">&#183;</font>
		<a id="reg" href="docs/VScv.pdf" target="_blank">CV</a> <font face="Symbol">&#183;</font>
		<a id="cv" href="#" onclick="toggle('email');">Email</a> 
	</div>
	<div id="email" style="display:none">
		smithv[ at ]cmu[ dot ]edu
	</div>
</div>

<!-- Dividing Line -->
<div id="headerline1">
	<hr class="linestyle" />
</div>

<!-- Image -->
<div id="mainimage">
	<img id="vsimg" src="img/g2.png" alt="Virginia Smith" width='160' height='160'/>
</div>

<!-- Bio -->
<div id="bio">
	<p>
		I'm an assistant professor in the Machine Learning Department at Carnegie Mellon University, and a courtesy faculty member in the Electrical and Computer Engineering Department.   
		My research interests are in machine learning, optimization, and distributed systems. 
		Recent topics include: large-scale machine learning, distributed optimization, federated and on-device learning, multi-task learning, and meta-learning. <br /><br />

		Prior to CMU, I was a postdoc with Chris R&eacute at Stanford University. I received my PhD at UC Berkeley, where I worked with Michael I. Jordan and David Culler as a member of the AMPLab.
	</p>
</div>

<!-- Dividing Line -->
<div id="headerline2">
	<hr class="linestyle" />
</div>

<!-- Recent News -->
<div id="news" class="mainbody">
<h2>Recent News</h2>
	<ul>
		<li>I enjoyed discussing some of our recent work on the <a href="https://twimlai.com/fairness-and-robustness-in-federated-learning-with-virginia-smith/" target="_blank">TWIML podcast</a></li>
		<li>I'm honored to be selected as one of MIT Technology Review's <a href="https://www.technologyreview.com/innovator/virginia-smith/" target="_blank">35 Innovators Under 35</a></li>
		<li>See our <a href="https://sites.google.com/view/fl-tutorial/" target="_blank">NeurIPS tutorial on Federated Learning</a></li>
		<li>I gave talks at the <a href="http://icfl.cc/SpicyFL/2020" target="_blank">NeurIPS SpicyFL Workshop</a> and <a href="https://www.youtube.com/watch?v=laCyJICLyWg" target="_blank">Stanford MLSys Seminar</a> on heterogeneous distributed learning</li>
		<li>I'm honored to receive a Facebook Faculty Award, and to join the <a href="https://www.private-ai.org/blog/" target="_blank">Intel/Avast/Borsetta Private AI Institute</a></li>
		<li>See our recent <a href="https://blog.ml.cmu.edu/2019/11/12/federated-learning-challenges-methods-and-future-directions/" target="_blank">blog post</a> and <a href="https://arxiv.org/abs/1908.07873" target="_blank">white paper</a> on federated learning</li>
	</ul>
</div> 


<!-- Dividing Line -->
<div id="headerline2">
	<hr class="linestyle" />
</div>

<!-- Students -->
<div id="students" class="mainbody">
<h2>PhD Students</h2>
	<ul>
		<li><a href="https://dkdennis.xyz/">Don Dennis</a></li>
		<li><a href="https://s-huu.github.io/">Shengyuan Hu</a></li>
		<li><a href="https://www.cs.cmu.edu/~mkuchnik/">Michael Kuchnik</a> (with <a href="http://users.ece.cmu.edu/~gamvrosi/">George Amvrosiadis</a>)</li>
		<li><a href="https://imkevinkuo.github.io/">Kevin Kuo</a></li>
		<li><a href="https://www.oscarli.one/">Oscar Li</a></li>
		<li><a href="http://www.cs.cmu.edu/~litian/">Tian Li</a></li>
		<li><a href="https://ars22.github.io/">Amrith Setlur</a></li>
	</ul>
</div> 

<!-- Dividing Line -->
<div id="headerline2">
	<hr class="linestyle" />
</div>

<!-- Teaching -->
<div id="teaching" class="mainbody">
	<h2>Teaching</h2>
	<ul>
		<li><a href="https://10605.github.io/" target="_blank">10-605/10-805</a> (PhD/MS ML with Large Datasets), Fall 2021</li>
		<li><a href="https://10605.github.io/spring2021/" target="_blank">10-405/10-605</a> (Undergraduate/MS ML with Large Datasets), Spring 2021</li>
		<li><a href="https://10605.github.io/fall2020/" target="_blank">10-605/10-805</a> (PhD/MS ML with Large Datasets), Fall 2020</li>
		<li><a href="https://10605.github.io/spring2020/" target="_blank">10-405/10-605</a> (Undergraduate/MS ML with Large Datasets), Spring 2020</li>
		<li><a href="https://18661.github.io/fall2018/index.html" target="_blank">18-461/18-661</a> (Intro to ML for Engineers), Fall 2018</a></li>
	</ul>
</div> 

<!-- Dividing Line -->
<div id="headerline2">
	<hr class="linestyle" />
</div>

<!-- Research -->
		<div id="research" class="mainbody">
		<h2>Publications</h2>
		<div id="pubwrapper">
			<br />
			<b>Preprints</b> 
			<br />
			<br />
			<div id="pubs">
				<div class="pub"><i>On Tilted Losses in Machine Learning: Theory and Applications</i><br />T. Li*, A. Beirami*, M. Sanjabi, V. Smith</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('term-journal')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/2109.06141" target="_blank">arxiv</a>]</div>
				<div class="abstract" id="term-journal" style="display:none">Exponential tilting is a technique commonly used in fields such as statistics, probability, information theory, and optimization to create parametric distribution shifts. Despite its prevalence in related fields, tilting has not seen widespread use in machine learning. In this work, we aim to bridge this gap by exploring the use of tilting in risk minimization. We study a simple extension to ERM -- tilted empirical risk minimization (TERM) -- which uses exponential tilting to flexibly tune the impact of individual losses. The resulting framework has several useful properties: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to a superquantile method. Our work makes rigorous connections between TERM and related objectives, such as Value-at-Risk, Conditional Value-at-Risk, and distributionally robust optimization (DRO). We develop batch and stochastic first-order optimization methods for solving TERM, provide convergence guarantees for the solvers, and show that the framework can be efficiently solved relative to common alternatives. Finally, we demonstrate that TERM can be used for a multitude of applications in machine learning, such as enforcing fairness between subgroups, mitigating the effect of outliers, and handling class imbalance. Despite the straightforward modification TERM makes to traditional ERM objectives, we find that the framework can consistently outperform ERM and deliver competitive performance with state-of-the-art, problem-specific approaches.
				</div>

				<div class="pub"><i>Private Multi-Task Learning: Formulation and Applications to Federated Learning</i><br />S. Hu, Z. Wu, V. Smith</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('dp-mtl')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/2108.12978" target="_blank">arxiv</a>]</div>
				<div class="abstract" id="dp-mtl" style="display:none">Many problems in machine learning rely on multi-task learning (MTL), in which the goal is to solve multiple related machine learning tasks simultaneously. MTL is particularly relevant for privacy-sensitive applications in areas such as healthcare, finance, and IoT computing, where sensitive data from multiple, varied sources are shared for the purpose of learning. In this work, we formalize notions of task-level privacy for MTL via joint differential privacy (JDP), a relaxation of differential privacy for mechanism design and distributed optimization. We then propose an algorithm for mean-regularized MTL, an objective commonly used for applications in personalized federated learning, subject to JDP. We analyze our objective and solver, providing certifiable guarantees on both privacy and utility. Empirically, we find that our method allows for improved privacy/utility trade-offs relative to global baselines across common federated learning benchmarks.
				</div>

				<div class="pub"><i>Label Leakage and Protection in Two-party Split Learning</i><br />O. Li, J. Sun, X. Yang, W. Gao, H. Zhang, J. Xie, V. Smith, C. Wang</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('split-learning')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/2102.08504" target="_blank">arxiv</a>]</div>
				<div class="abstract" id="split-learning" style="display:none">Two-party split learning is a popular technique for learning a model across feature-partitioned data. In this work, we explore whether it is possible for one party to steal the private label information from the other party during split training, and whether there are methods that can protect against such attacks. Specifically, we first formulate a realistic threat model and propose a privacy loss metric to quantify label leakage in split learning. We then show that there exist two simple yet effective methods within the threat model that can allow one party to accurately recover private ground-truth labels owned by the other party. To combat these attacks, we propose several random perturbation techniques, including Marvell, an approach that strategically finds the structure of the noise perturbation by minimizing the amount of label leakage (measured through our quantification metric) of a worst-case adversary. We empirically demonstrate the effectiveness of our protection techniques against the identified attacks, and show that Marvell in particular has improved privacy-utility tradeoffs relative to baseline approaches.			
				</div>


				<div class="pub"><i>A Field Guide to Federated Optimization</i><br />J. Wang, Z. Charles, Z. Xu, G. Joshi, H. B. McMahan, et al. <br /></div>
				<div class="links">[<a class="publinks" href="javascript:toggle('fedoptguide')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/2107.06917" target="_blank">arxiv</a>]</div>
				<div class="abstract" id="fedoptguide" style="display:none">Federated learning and analytics are a distributed approach for collaboratively learning models (or statistics) from decentralized data, motivated by and designed for privacy protection. The distributed learning process can be formulated as solving federated optimization problems, which emphasize communication efficiency, data heterogeneity, compatibility with privacy and system requirements, and other constraints that are not primary considerations in other problem settings. This paper provides recommendations and guidelines on formulating, designing, evaluating and analyzing federated optimization algorithms through concrete examples and practical implementation, with a focus on conducting effective simulations to infer real-world performance. The goal of this work is not to survey the current literature, but to inspire researchers and practitioners to design federated learning algorithms that can be used in various practical applications.
				</div>
		</div>

		<b>2021</b>
		<br />
		<br />

		<div id="pubs">
				<div class="pub"><i>On Large-Cohort Training for Federated Learning</i><br />Z. Charles, Z. Garrett, Z. Huo, S. Shmulyian, V. Smith<br />Neural Information Processing Systems (NeurIPS), 2021</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('large-cohort')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/2106.07820" target="_blank">arxiv</a>]</div>
				<div class="abstract" id="large-cohort" style="display:none">Federated learning methods typically learn a model by iteratively sampling updates from a population of clients. In this work, we explore how the number of clients sampled at each round (the cohort size) impacts the quality of the learned model and the training dynamics of federated learning algorithms. Our work poses three fundamental questions. First, what challenges arise when trying to scale federated learning to larger cohorts? Second, what parallels exist between cohort sizes in federated learning and batch sizes in centralized learning? Last, how can we design federated learning methods that effectively utilize larger cohort sizes? We give partial answers to these questions based on extensive empirical evaluation. Our work highlights a number of challenges stemming from the use of larger cohorts. While some of these (such as generalization issues and diminishing returns) are analogs of large-batch training challenges, others (including training failures and fairness concerns) are unique to federated learning.</div>

				<div class="pub"><i>Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing</i><br />M. Khodak, R. Tu, T. Li, L. Li, M.-F. Balcan, V. Smith, A. Talwalkar<br />Neural Information Processing Systems (NeurIPS), 2021</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('fedex')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/2106.04502" target="_blank">arxiv</a>] [<a class="publinks" href=
				"https://github.com/mkhodak/FedEx" target="_blank">code</a>]</div>
				<div class="abstract" id="fedex" style="display:none">Tuning hyperparameters is a crucial but arduous part of the machine learning pipeline. Hyperparameter optimization is even more challenging in federated learning, where models are learned over a distributed network of heterogeneous devices; here, the need to keep data on device and perform local training makes it difficult to efficiently train and evaluate configurations. In this work, we investigate the problem of federated hyperparameter tuning. We first identify key challenges and show how standard approaches may be adapted to form baselines for the federated setting. Then, by making a novel connection to the neural architecture search technique of weight-sharing, we introduce a new method, FedEx, to accelerate federated hyperparameter tuning that is applicable to widely-used federated optimization methods such as FedAvg and recent variants. Theoretically, we show that a FedEx variant correctly tunes the on-device learning rate in the setting of online convex optimization across devices. Empirically, we show that FedEx can outperform natural baselines for federated hyperparameter tuning by several percentage points on the Shakespeare, FEMNIST, and CIFAR-10 benchmarks, obtaining higher accuracy using the same training budget.</div>

				<div class="pub"><i>Two Sides of Meta-Learning Evaluation: In vs. Out of Distribution</i><br />A. Setlur*, O. Li*, V. Smith<br />Neural Information Processing Systems (NeurIPS), 2021</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('fixml')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/2102.11503" target="_blank">arxiv</a>]</div>
				<div class="abstract" id="fixml" style="display:none">We categorize meta-learning evaluation into two settings: in-distribution [ID], in which the train and test tasks are sampled iid from the same underlying task distribution, and out-of-distribution [OOD], in which they are not. While most meta-learning theory and some FSL applications follow the ID setting, we identify that most existing few-shot classification benchmarks instead reflect OOD evaluation, as they use disjoint sets of train (base) and test (novel) classes for task generation. This discrepancy is problematic because -- as we show on numerous benchmarks -- meta-learning methods that perform better on existing OOD datasets may perform significantly worse in the ID setting. In addition, in the OOD setting, even though current FSL benchmarks seem befitting, our study highlights concerns in 1) reliably performing model selection for a given meta-learning method, and 2) consistently comparing the performance of different methods. To address these concerns, we provide suggestions on how to construct FSL benchmarks to allow for ID evaluation as well as more reliable OOD evaluation. Our work aims to inform the meta-learning community about the importance and distinction of ID vs. OOD evaluation, as well as the subtleties of OOD evaluation with current benchmarks.</div>

				<div class="pub"><i>Progressive Compressed Records: Taking a Byte out of Deep Learning Data</i><br />M. Kuchnik, G. Amvrosiadis, V. Smith<br />Conference on Very Large Data Bases (VLDB), 2021</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('pcr')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/1911.00472" target="_blank">arxiv</a>] [<a class="publinks" href=
				"https://github.com/mkuchnik/PCR_Release" target="_blank">code</a>]</div>
				<div class="abstract" id="pcr" style="display:none">Deep learning training accesses vast amounts of data at high velocity, posing challenges for datasets retrieved over commodity networks and storage devices. We introduce a way to dynamically reduce the overhead of fetching and transporting training data with a method we term Progressive Compressed Records (PCRs). PCRs deviate from previous formats by using progressive compression to convert a single dataset into multiple datasets of increasing fidelity---all without adding to the total dataset size. Empirically, we implement PCRs and evaluate them on a wide range of datasets: ImageNet, HAM10000, Stanford Cars, and CelebA-HQ. Our results show that different tasks can tolerate different levels of compression. PCRs use an on-disk layout that enables applications to efficiently and dynamically access appropriate levels of compression at runtime. In turn, we demonstrate that PCRs can seamlessly enable a 2x speedup in training time on average over baseline formats.</div>

				<div class="pub"><i>Ditto: Fair and Robust Federated Learning Through Personalization</i><br />T. Li, S. Hu, A. Beirami, V. Smith<br />International Conference on Machine Learning (ICML), 2021<br /><b>Best Paper Award at ICLR 2021 Secure ML Workshop</b></div>
				<div class="links">[<a class="publinks" href="javascript:toggle('ditto')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/2012.04221" target="_blank">arxiv</a>] [<a class="publinks" href=
				"https://github.com/litian96/ditto" target="_blank">code</a>]</div>
				<div class="abstract" id="ditto" style="display:none">Fairness and robustness are two important concerns for federated learning systems. In this work, we identify that robustness to data and model poisoning attacks and fairness, measured as the uniformity of performance across devices, are competing constraints in statistically heterogeneous networks. To address these constraints, we propose employing a simple, general framework for personalized federated learning, Ditto, and develop a scalable solver for it. Theoretically, we analyze the ability of Ditto to achieve fairness and robustness simultaneously on a class of linear problems. Empirically, across a suite of federated datasets, we show that Ditto not only achieves competitive performance relative to recent personalization methods, but also enables more accurate, robust, and fair models relative to state-of-the-art fair or robust baselines.</div>

				<div class="pub"><i>Heterogeneity for the Win: One-Shot Federated Clustering</i><br />D. Dennis, T. Li, V. Smith<br />International Conference on Machine Learning (ICML), 2021</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('kfed')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/2103.00697" target="_blank">arxiv</a>]</div>
				<div class="abstract" id="kfed" style="display:none">In this work, we explore the unique challenges---and opportunities---of unsupervised federated learning (FL). We develop and analyze a one-shot federated clustering scheme, k-FED, based on the widely-used Lloyd's method for k-means clustering. In contrast to many supervised problems, we show that the issue of statistical heterogeneity in federated networks can in fact benefit our analysis. We analyse k-FED under a center separation assumption and compare it to the best known requirements of its centralized counterpart. Our analysis shows that in heterogeneous regimes where the number of clusters per device (k') is smaller than the total number of clusters over the network k, ($k' \le \sqrt{k}$), we can use heterogeneity to our advantage---significantly weakening the cluster separation requirements for k-FED. From a practical viewpoint, k-FED also has many desirable properties: it requires only round of communication, can run asynchronously, and can handle partial participation or node/network failures. We motivate our analysis with experiments on common FL benchmarks, and highlight the practical utility of one-shot clustering through use-cases in personalized FL and device sampling.</div>

				<div class="pub"><i>Tilted Empirical Risk Minimization</i><br />T. Li*, A. Beirami*, M. Sanjabi, V. Smith<br />International Conference on Learning Representations (ICLR), 2021</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('term')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/2007.01162" target="_blank">arxiv</a>] [<a class="publinks" href="https://github.com/litian96/TERM" target="_blank">code</a>] [<a class="blog" href="https://blog.ml.cmu.edu/2021/04/02/term/" target="_blank">blog post</a>]</div>
				<div class="abstract" id="term" style="display:none">Empirical risk minimization (ERM) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly. While many methods aim to address these problems individually, in this work, we explore them through a unified framework---tilted empirical risk minimization (TERM). In particular, we show that it is possible to flexibly tune the impact of individual losses through a straightforward extension to ERM using a hyperparameter called the tilt. We provide several interpretations of the resulting framework: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to a superquantile method. We develop batch and stochastic first-order optimization methods for solving TERM, and show that the problem can be efficiently solved relative to common alternatives. Finally, we demonstrate that TERM can be used for a multitude of applications, such as enforcing fairness between subgroups, mitigating the effect of outliers, and handling class imbalance. TERM is not only competitive with existing solutions tailored to these individual problems, but can also enable entirely new applications, such as simultaneously addressing outliers and promoting fairness.</div>
		</div>
		<b>2020</b>
		<br />
		<br />
		<div id="pubs">
				<div class="pub"><i>Federated Learning: Challenges, Methods, and Future Directions</i><br />T. Li, A. K. Sahu, A. Talwalkar, V. Smith<br />IEEE Signal Processing Magazine, Special Issue on Distributed Machine Learning, 2020</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('flsurvey')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/1908.07873" target="_blank">arxiv</a>] [<a class="blog" href="https://blog.ml.cmu.edu/2019/11/12/federated-learning-challenges-methods-and-future-directions/" target="_blank">blog post</a>]</div>
				<div class="abstract" id="flsurvey" style="display:none">Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.</div>


				<div class="pub"><i>Fair Resource Allocation in Federated Learning</i><br />T. Li, M. Sanjabi, A. Beirami, V. Smith<br />International Conference on Learning Representations (ICLR), 2020</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('fairflearn')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/1905.10497" target="_blank">arxiv</a>] [<a class="publinks" href="https://github.com/litian96/fair_flearn" target="_blank">code</a>]</div>
				<div class="abstract" id="fairflearn" style="display:none">Federated learning involves training statistical models in massive, heterogeneous networks. Naively minimizing an aggregate loss function in such a network may disproportionately advantage or disadvantage some of the devices. In this work, we propose q-Fair Federated Learning (q-FFL), a novel optimization objective inspired by resource allocation in wireless networks that encourages a more fair (i.e., lower-variance) accuracy distribution across devices in federated networks. To solve q-FFL, we devise a communication-efficient method, q-FedAvg, that is suited to federated networks. We validate both the effectiveness of q-FFL and the efficiency of q-FedAvg on a suite of federated datasets, and show that q-FFL (along with q-FedAvg) outperforms existing baselines in terms of the resulting fairness, flexibility, and efficiency.</div>
				
				<div class="pub"><i>Learning Context-aware Policies from Multiple Smart Homes via Federated Multi-Task Learning</i><br />T. Yu, T. Li, Y. Sun, S. Nanda, V. Smith, V. Sekar, S. Seshan<br />ACM/IEEE Conference on Internet of Things Design and Implementation (IoTDI), 2020</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('iotdi')">abstract</a>]</div>
				<div class="abstract" id="iotdi" style="display:none">Internet-of-Things (IoT) devices deployed in smart homes expose users to cyber threats that can cause privacy leakage (e.g., smart TV eavesdropping) or physical hazards (e.g., smart stove causing fire). Prior work has argued that to effectively detect and prevent such threats, contextual policies are needed to decide if an access to an IoT device should be allowed. Today, however, such contextual access control policies need to be manually generated by IoT developers or users via preinstallation or runtime prompts. Both approaches suffer from potential misconfigurations and often fail to provide coverage over the space of policies. In this paper, our goal is to build a machine learning framework to automatically learn the contextual access control policies from the observed behavioral patterns of users in smart homes. Designing such a learning framework is challenging on two fronts. First, the accuracy is constrained by insufficient data in some smart homes and the diversity of IoT access patterns across different smart homes. Second, since we rely on usage patterns of IoT devices, users will have privacy concerns. We address these challenges in designing LoFTI, a federated multi-task learning framework that learns customized context-aware policies from multiple smart homes in a privacy-preserving manner. Based on prior user studies, we identify six general types of features to capture contextual access patterns. We build a simple machine learning model with temporal structure to achieve a good trade-off between accuracy and communication/computation cost. We design a custom data augmentation mechanism to address the issue of unbalanced data in learning (i.e., few negative vs. normal samples). We show that LoFTI can achieve low false positives/false negatives, reducing the false negative rate by 24.2% and false positive rate by 49.5%, comparing with the state-of-the-art single-home learning and all-home learning mechanism.</div>	


				<div class="pub"><i>Federated Optimization in Heterogeneous Networks</i><br />T. Li, A. K. Sahu, M. Sanjabi, M. Zaheer, A. Talwalkar, V. Smith<br />Conference on Machine Learning and Systems (MLSys), 2020</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('fedprox')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/1812.06127" target="_blank">arxiv</a>] [<a class="publinks" href="https://github.com/litian96/FedProx" target="_blank">code</a>]</div>
				<div class="abstract" id="fedprox" style="display:none">Federated learning involves training machine learning models in massively distributed networks. While Federated Averaging (FedAvg) is the leading optimization method for training non-convex models in this setting, its behavior is not well understood in realistic federated settings when learning across statistically heterogeneous devices, i.e., where each device collects data in a non-identical fashion. In this work, we introduce a framework to tackle statistical heterogeneity, FedProx, which encompasses FedAvg as a special case. We provide convergence guarantees for FedProx through a novel device dissimilarity assumption, which allows us to characterize heterogeneity in the network. Finally, we perform a detailed empirical evaluation across a suite of federated datasets, validating our theoretical analysis and demonstrating the improved robustness and stability of the generalized FedProx framework relative to FedAvg for learning in heterogeneous networks.</div>	
		</div>
		<b>2019</b>
		<br />
		<br />
		<div id="pubs">
				<div class="pub"><i>LEAF: A Benchmark for Federated Settings</i><br />S. Caldas, P. Wu, T. Li, J. Konecny, B. McMahan, V. Smith, A. Talwalkar<br />Workshop on Federated Learning for Data Privacy and Confidentiality at NeurIPS, 2019</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('leaf')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/1812.01097" target="_blank">arxiv</a>] [<a class="publinks" href="https://talwalkarlab.github.io/leaf/" target="_blank">code</a>]</div>
				<div class="abstract" id="leaf" style="display:none">Modern federated networks, such as those comprised of wearable devices, mobile phones, or autonomous vehicles, generate massive amounts of data each day. This wealth of data can help to learn models that can improve the user experience on each device. However, learning in federated settings presents new challenges at all stages of the machine learning pipeline. As the machine learning community begins to tackle these challenges, we are at a critical time to ensure that developments made in this area are grounded in real-world assumptions. To this end, we propose LEAF, a modular benchmarking framework for learning in federated settings. LEAF includes a suite of open-source federated datasets, a rigorous evaluation framework, and a set of reference implementations, all geared towards capturing the obstacles and intricacies of practical federated environments.</div>	

				<div class="pub"><i>FedDANE: A Federated Newton-Type Method</i><br />T. Li, A. K. Sahu, M. Sanjabi, M. Zaheer, A. Talwalkar, V. Smith<br />Asilomar Conference on Signals, Systems and Computers, 2019, Invited Paper</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('feddane')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/2001.01920" target="_blank">arxiv</a>]</div>
				<div class="abstract" id="feddane" style="display:none">Federated learning aims to jointly learn statistical models over massively distributed remote devices. In this work, we propose FedDANE, an optimization method that we adapt from DANE, a method for classical distributed optimization, to handle the practical constraints of federated learning. We provide convergence guarantees for this method when learning over both convex and non-convex functions. Despite encouraging theoretical results, we find that the method has underwhelming performance empirically. In particular, through empirical simulations on both synthetic and real-world datasets, FedDANE consistently underperforms baselines of FedAvg and FedProx in realistic federated settings. We identify low device participation and statistical device heterogeneity as two underlying causes of this underwhelming performance, and conclude by suggesting several directions of future work.</div>

				<!-- augmentation -->
				<div class="pub"><i>A Kernel Theory of Modern Data Augmentation</i><br />T. Dao, A. Gu, A. Ratner, V. Smith, C. De Sa, C. Re <br />International Conference on Machine Learning (ICML), 2019</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('augmentation')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/1803.06084" target="_blank">arxiv</a>] [<a class="publinks" href="https://github.com/HazyResearch/augmentation_code" target="_blank">code</a>]</div>
				<div class="abstract" id="augmentation" style="display:none">Data augmentation, a technique in which a training set is expanded with class-preserving transformations, is ubiquitous in modern machine learning pipelines. In this paper, we seek to establish a theoretical framework for understanding modern data augmentation techniques. We start by showing that for kernel classifiers, data augmentation can be approximated by first-order feature averaging and second-order variance regularization components. We connect this general approximation framework to prior work in invariant kernels, tangent propagation, and robust optimization. Next, we explicitly tackle the compositional aspect of modern data augmentation techniques, proposing a novel model of data augmentation as a Markov process. Under this model, we show that performing k-nearest neighbors with data augmentation is asymptotically equivalent to a kernel classifier. Finally, we illustrate ways in which our theoretical framework can be leveraged to accelerate machine learning workflows in practice, including reducing the amount of computation needed to train on augmented data, and predicting the utility of a transformation prior to training.</div>

				<div class="pub"><i>Efficient Augmentation via Data Subsampling</i><br />M. Kuchnik, V. Smith<br />International Conference on Learning Representations (ICLR), 2019</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('effaug')">abstract</a>] [<a class="publinks" href="https://openreview.net/pdf?id=Byxpfh0cFm" target="_blank">pdf</a>] [<a class="publinks" href="https://github.com/mkuchnik/Efficient_Augmentation" target="_blank">code</a>]</div>
				<div class="abstract" id="effaug" style="display:none">Data augmentation is commonly used to encode invariances in learning methods. However, this process is often performed in an inefficient manner, as artificial examples are created by applying a number of transformations to all points in the training set. The resulting explosion of the dataset size can be an issue in terms of storage and training costs, as well as in selecting and tuning the optimal set of transformations to apply. In this work, we demonstrate that it is possible to significantly reduce the number of data points included in data augmentation while realizing the same accuracy and invariance benefits of augmenting the entire dataset. We propose a novel set of subsampling policies, based on model influence and loss, that can achieve a 90% reduction in augmentation set size while maintaining the accuracy gains of standard data augmentation.</div>

				<div class="pub"><i>MLSys: The New Frontier of Machine Learning Systems</i><br />Technical Report, 2019</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('mlsys')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/1904.03257" target="_blank">arxiv</a>]</div>
				<div class="abstract" id="mlsys" style="display:none">Machine learning (ML) techniques are enjoying rapidly increasing adoption. However, designing and implementing the systems that support ML models in real-world deployments remains a significant obstacle, in large part due to the radically different development and deployment profile of modern ML methods, and the range of practical concerns that come with broader adoption. We propose to foster a new systems machine learning research community at the intersection of the traditional systems and ML communities, focused on topics such as hardware systems for ML, software systems for ML, and ML optimized for metrics beyond predictive accuracy. To do this, we describe a new conference, MLSys, that explicitly targets research at the intersection of systems and machine learning with a program committee split evenly between experts in systems and ML, and an explicit focus on topics at the intersection of the two.</div>
		</div>
		<b>2018 &#38; prior</b>
		<br />
		<br />
		<div id="pubs">
				<!-- proxcocoa -->
				<div class="pub"><i>CoCoA: A General Framework for Communication-Efficient Distributed Optimization</i><br /> V. Smith, S. Forte, C. Ma, M. Takac, M. I. Jordan, M. Jaggi<br />Journal of Machine Learning Research (JMLR), 2018</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('cocoajournal')">abstract</a>] [<a class="publinks" href="http://www.jmlr.org/papers/volume18/16-512/16-512.pdf" target="_blank">pdf</a>] [<a class="publinks" href="https://github.com/gingsmith/proxcocoa" target="_blank">code</a>]</div> <!--[<a class="publinks" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/sdca_ops.cc" target="_blank">tensorflow</a>]-->
				<div class="abstract" id="cocoajournal" style="display:none">The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for the distributed environment, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the-art methods, as we illustrate with an extensive set of experiments on real distributed datasets.</div>

				<!-- oneshot -->
				<div class="pub"><i>One-Shot Federated Learning</i><br /> N. Guha, A. Talwalkar, V. Smith <br />Machine Learning on Devices Workshop at NeurIPS, 2018</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('oneshot')">abstract</a>] [<a class="publinks" href="https://arxiv.org/abs/1902.11175" target="_blank">arxiv</a>]</div>
				<div class="abstract" id="oneshot" style="display:none">We present one-shot federated learning, where a central server learns a global model over a network of federated devices in a single round of communication. Our approach - drawing on ensemble learning and knowledge aggregation - achieves an average relative gain of 51.5% in AUC over local baselines and comes within 90.1% of the (unattainable) global ideal. We discuss these methods and identify several promising directions of future work.</div>

				<!-- flearn -->
				<div class="pub"><i>Federated Multi-Task Learning</i><br /> V. Smith, C. Chiang, M. Sanjabi, A. Talwalkar<br />Neural Information Processing Systems (NeurIPS), 2017</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('flearn')">abstract</a>] [<a class="publinks" href="https://papers.nips.cc/paper/7029-federated-multi-task-learning" target="_blank">pdf</a>] [<a class="publinks" href="https://github.com/gingsmith/fmtl" target="_blank">code</a>]</div>
				<div class="abstract" id="flearn" style="display:none">Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.</div>

				<!-- cocoa+ journal -->
				<div class="pub"><i>Distributed Optimization with Arbitrary Local Solvers</i> <br /> C. Ma, J. Konecny, M. Jaggi, V. Smith, M. I. Jordan, P. Richtarik, M. Takac <br /> Optimization Methods and Software, 2017</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('cocoapj')">abstract</a>] [<a class="publinks" href="http://www.tandfonline.com/doi/full/10.1080/10556788.2016.1278445" target="_blank">pdf</a>]</div>
				<div class="abstract" id="cocoapj" style="display:none">With the growth of data and necessity for distributed optimization methods, solvers that work well on a single machine must be re-designed to leverage distributed computation. Recent work in this area has been limited by focusing heavily on developing highly specific methods for the distributed environment. These special-purpose methods are often unable to fully leverage the competitive performance of their well-tuned and customized single machine counterparts. Further, they are unable to easily integrate improvements that continue to be made to single machine methods. To this end, we present a framework for distributed optimization that both allows the flexibility of arbitrary solvers to be used on each (single) machine locally, and yet maintains competitive performance against other state-of-the-art special-purpose distributed methods. We give strong primal-dual convergence rate guarantees for our framework that hold for arbitrary local solvers. We demonstrate the impact of local solver selection both theoretically and in an extensive experimental comparison. Finally, we provide thorough implementation details for our framework, highlighting areas for practical performance gains.</div>

				<!-- proxcocoa -->
				<div class="pub"><i>L1-Regularized Distributed Optimization: A Communication-Efficient Primal-Dual Framework</i><br /> V. Smith, S. Forte, M. I. Jordan, M. Jaggi <br />ML Systems Workshop at ICML, 2016</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('proxcocoa')">abstract</a>] [<a class="publinks" href="http://arxiv.org/abs/1512.04011" target="_blank">arxiv</a>] [<a class="publinks" href="https://github.com/gingsmith/proxcocoa" target="_blank">code</a>]</div>
				<div class="abstract" id="proxcocoa" style="display:none">Despite the importance of sparsity in many big data applications, there are few existing methods for efficient distributed optimization of sparsely-regularized objectives. In this paper, we present a communication-efficient framework for L1-regularized optimization in distributed environments. By taking a non-traditional view of classical objectives as part of a more general primal-dual setting, we obtain a new class of methods that can be efficiently distributed and is applicable to common L1-regularized regression and classification objectives, such as Lasso, sparse logistic regression, and elastic net regression. We provide convergence guarantees for this framework and demonstrate strong empirical performance as compared to other state-of-the-art methods on several real-world distributed datasets.</div>

				<!-- longform -->
				<div class="pub"><i>Going In-Depth: Finding Longform on the Web </i><br /> V. Smith, M. Connor, I. Stanton<br /> Conference on Knowledge Discovery and Data Mining (KDD), 2015 </div>
				<div class="links">[<a class="publinks" href="javascript:toggle('indepth')">abstract</a>] [<a class="publinks" href="http://dl.acm.org/citation.cfm?id=2788599" target="_blank">pdf</a>]</div>
				<div class="abstract" id="indepth" style="display:none">tl;dr: Longform articles are extended, in-depth pieces that often serve as feature stories in newspapers and magazines. In this work, we develop a system to automatically identify longform content across the web. Our novel classifier is highly accurate despite huge variation within longform in terms of topic, voice, and editorial taste. It is also scalable and interpretable, requiring a surprisingly small set of features based only on language and parse structures, length, and document interest. We implement our system at scale and use it to identify a corpus of several million longform documents. Using this corpus, we provide the first web-scale study with quantifiable and measurable information on longform, giving new insight into questions posed by the media on the past and current state of this famed literary medium.</div>

				<!-- cocoa+ -->
				<div class="pub"><i>Adding vs. Averaging in Distributed Primal-Dual Optimization</i> <br /> C. Ma*, V. Smith*, M. Jaggi, M. I. Jordan, P. Richtarik, M. Takac <br />International Conference on Machine Learning (ICML), 2015</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('cocoap')">abstract</a>] [<a class="publinks" href="http://proceedings.mlr.press/v37/mab15.pdf" target="_blank">pdf</a>] [<a class="publinks" href="http://gingsmith.github.io/cocoa/" target="_blank">code</a>]</div>
				<div class="abstract" id="cocoap" style="display:none">Distributed optimization methods for large-scale machine learning suffer from a communication bottleneck. It is difficult to reduce this bottleneck while still efficiently and accurately aggregating partial work from different machines. In this paper, we present a novel generalization of the recent communication-efficient primal-dual framework (CoCoA) for distributed optimization. Our framework, CoCoA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes only allow conservative averaging. We give stronger (primal-dual) convergence rate guarantees for both CoCoA as well as our new variants, and generalize the theory for both methods to cover non-smooth convex loss functions. We provide an extensive experimental comparison that shows the markedly improved performance of CoCoA+ on several real-world distributed datasets, especially when scaling up the number of machines.</div>

				<!-- cocoa -->
				<div class="pub"><i>Communication-Efficient Distributed Dual Coordinate Ascent </i> <br /> M. Jaggi*, V. Smith*, M. Takac, J. Terhorst, S. Krishnan, T. Hofmann, M. I. Jordan <br />Neural Information Processing Systems (NeurIPS), 2014</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('cocoa')">abstract</a>] [<a class="publinks" href="docs/cocoa.pdf" target="_blank">pdf</a>] [<a class="publinks" href="http://gingsmith.github.io/cocoa/" target="_blank">code</a>]</div>
				<div class="abstract" id="cocoa" style="display:none">Communication remains the most significant bottleneck in the performance of distributed optimization algorithms for large-scale machine learning. In this paper, we propose a communication-efficient framework, COCOA, that uses local computation in a primal-dual setting to dramatically reduce the amount of necessary communication. We provide a strong convergence rate analysis for this class of algorithms, as well as experiments on real-world distributed datasets with implementations in Spark. In our experiments, we find that as compared to state-of-the-art mini-batch versions of SGD and SDCA algorithms, COCOA converges to the same .001-accurate solution quality on average 25x as quickly.</div>

				<!-- MLI -->
				<div class="pub"><i>MLI: An API for User-friendly Distribued Machine Learning</i><br /> E. Sparks, A. Talwalkar, V. Smith, X. Pan, J. Gonzalez, T. Kraska, M. I. Jordan, and M. J. Franklin <br />IEEE International Conference on Data Mining (ICDM), 2013</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('icdm13')">abstract</a>] [<a class="publinks" href="docs/mlinterface_icdm_2013.pdf" target="_blank">pdf</a>]</div>
				<div class="abstract" id="icdm13" style="display:none">MLI is a Application Programming Interface designed to address the challenges of building Machine Learning algorithms in a distributed setting based on data-centric computing. Its primary goal is to simplify the development of high-performance, scalable, distributed algorithms. Our initial results show that, relative to existing systems, this interface can be used to build distributed implementations of a wide variety of common Machine Learning algorithms with minimal complexity and highly competitive performance and scalability.</div>


				<!-- Renewables -->	
				<div class="pub"><i>A Comparative Study of High Renewables Penetration Electricity Grids</i> <br /> J. Taneja, V. Smith, D. Culler, and C. Rosenberg <br />IEEE International Conference on Smart Grid Communications (SmartGridComm), 2013</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('smartgridcomm')">abstract</a>] [<a class="publinks" href="docs/renewables_sgc_2013.pdf" target="_blank">pdf</a>] </div>
				<div class="abstract" id="smartgridcomm" style="display:none">Electricity grids are transforming as renewables proliferate, yet operational concerns due to fluctuations in renewables sources could limit the ultimate potential for high penetrations of renewables. In this paper, we compare three electricity grids  California, Germany, and Ontario  studying the effects of relative cost of solar and wind generation on the selection of the renewables mix, and examine the resulting excess generation. We then observe the effects of the renewables mix and the use of baseload energy generation on the limits to renewables penetration, quantifying what proportion of delivered energy can be provided by renewables. Our study shows that the optimal renewables mix, from the perspective of minimizing total cost of generation, is highly dependent on the relative costs of technology, and that above a certain penetration rate, different for each grid, the optimal mix must contain both solar and wind generation.</div>

				<!-- Sidewalks -->
				<div class="pub"><i>Classification of Sidewalks in Street View Images</i> <br /> V. Smith, J. Malik, and D. Culler <br />WiP Workshop at International Green Computing Conference (IGCC), 2013</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('igcc12')">abstract</a>]</div>
				<div class="abstract" id="igcc12" style="display:none">Mapping sidewalks in urban environments is key in the creation of pedestrian-friendly, sustainable cities. Currently, urban planners are hindered by a lack of information available in a format suitable for the large-scale analysis of sidewalk design. To demonstrate the impact that information technology could have in this area, we leverage techniques from machine learning and computer vision to gather information about the presence and quality of sidewalks in map images. In particular, we identify sidewalk segments in street view images using a random forest classifier, utilizing a set of local and global features that include geometric context, presence of lanes, pixel color, and location. Our results illustrate that this approach is effective in classifying sidewalk segments in a large set of street view images. This algorithm can be easily extended to other datasets, and can be automated to gather complete, fine-grained details about sidewalks for arbitrarily large urban environments.</div>


				<!-- MLbase -->
				<div class="pub"><i>MLbase: A Distributed Machine Learning Wrapper </i> <br /> A. Talwalkar, T. Kraska, R. Griffith, J. Duchi, J. Gonzalez, D. Britz, X. Pan, V. Smith, E. Sparks, A. Wibisono, M. J. Franklin, and M. I. Jordan <br />Big Learning Workshop at NeurIPS, 2012</div><div class="links">[<a class="publinks" href="javascript:toggle('nips12')">abstract</a>] [<a class="publinks" href="docs/mlbase_nips_2012.pdf" target="_blank">pdf</a>]</div>
				<div class="abstract" id="nips12" style="display:none"> Machine learning (ML) and statistical techniques are key to transforming big data into actionable knowledge. In spite of the modern primacy of data, the complexity of existing ML algorithms is often overwhelmingany users do not understand the trade-offs and challenges of parameterizing and choosing between different learning techniques. Furthermore, existing scalable systems that support machine learning are typically not accessible to ML researchers without a strong background in distributed systems and low-level primitives. In this work, we present our vision for MLbase, a novel system harnessing the power of machine learning for both end-users and ML researchers. MLbase provides (1) a simple declarative way to specify ML tasks, (2) a novel optimizer to select and dynamically adapt the choice of learning algorithm, (3) a set of high-level operators to enable ML researchers to scalably implement a wide range of ML methods without deep systems knowledge, and (4) a new run-time optimized for the data-access patterns of these high-level operators.</div>

				<!-- HVAC ACC-->
				<div class="pub"><i>Identifying Models of HVAC Systems Using Semiparametric Regression</i> <br /> A. Aswani, N. Master, J. Taneja, V. Smith, A. Krioukov, D. Culler, and C. Tomlin <br /> Proceedings of the American Control Conference (ACC), 2012</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('acc')">abstract</a>] [<a class="publinks" href="docs/brite_acc_2012.pdf" target="_blank">pdf</a>]</div>
				<div class="abstract" id="acc" style="display:none">Heating, ventilation, and air-conditioning (HVAC) systems use a large amount of energy, and so they are an interesting area for efficiency improvements. The focus here is on the use of semiparametric regression to identify models, which are amenable to analysis and control system design, of HVAC systems. This paper briefly describes two testbeds that we have built on the Berkeley campus for modeling and efficient control of HVAC systems, and we use these testbeds as case studies for system identification. The main contribution of this work is that the use of semiparametric regression allows for the estimation of the heating load from occupancy, equipment, and solar heating using only temperature measurements. These estimates are important for building accurate models as well as designing efficient control schemes, and in our other work we have been able to achieve a reduction in energy consumption on a single room testbed using heating load estimation in conjunction with the learning-based model predictive control (LBMPC) technique. Furthermore, this framework is not restrictive to modeling nonlinear HVAC behavior, because we have been able to use this methodology to create hybrid system models that incorporate such nonlinearities.</div>


				<!-- HVAC SIGBED -->
				<div class="pub"><i>Modeling Building Thermal Response to HVAC Zoning</i><br /> V. Smith, T. Sookoor, and K. Whitehouse <br />ACM SIGBED Review, 2012</div>
				<div class="links">[<a class="publinks" href="javascript:toggle('sigbed')">abstract</a>] [<a class="publinks" href="docs/hvac_conet_2012.pdf" target="_blank">pdf</a>]</div>
				<div class="abstract" id="sigbed" style="display:none"> HVAC systems account for 38% of building energy usage. Studies have indicated at least 5-15% waste due to unoccupied spaces being conditioned. Our goal is to minimize this waste by retrofitting HVAC systems to enable <i>room-level zoning</i> where each room is conditioned individually based on its occupancy. This will allow only occupied rooms to be conditioned while saving the energy used to condition unoccupied rooms. In order to achieve this goal, the effect of opening or closing air vent registers on room temperatures has to be predicted. Making such a prediction is complicated by the fact that weather has a larger effect on room temperatures than the settings of air vent registers, making it hard to isolate the influence of the HVAC system. We present a technique for dynamically estimating the heat load due to weather on room temperatures and subtracting it out in order to predict the effect of the HVAC system more directly.</div>

			</div>
		</div>
		</div>

<!-- push needed for sticky footer -->
<!--<div id="push"></div>
</div>-->

<!-- STICKY FOOTER -->
<!--<div id="footerline">
	<hr class="linestyle" />
</div>
<div id="footer">
	<p> &copy 2020 Virginia Smith. All rights reserved. </p>
</div>-->

</body>
</html>
